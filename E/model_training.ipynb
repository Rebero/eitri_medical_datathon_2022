{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bergen Datathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the readmission dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'bed_day_dep', 'DRG', 'gender',\n",
       "       'timestamp_in', 'timestamp_out', 'degree_of_urgency', 'patientID',\n",
       "       'hosp_unique_ID', 'mainDiagICD10Chapter', 'mainDiagBlock',\n",
       "       'diagnosis_code_1', 'age_group', 'bed_day_hosp', 'readmission', 'mors',\n",
       "       'lead_to_readmission'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that will not be used for prediction and set target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 1\n",
    "features_not_predictive = ['Unnamed: 0','Unnamed: 0.1', 'bed_day_dep', 'timestamp_in', 'timestamp_out', 'hosp_unique_ID', 'mainDiagICD10Chapter', 'diagnosis_code_1', 'readmission', 'mors']\n",
    "\n",
    "# category 2\n",
    "features_to_remove = []\n",
    "# category 3 (the rest)\n",
    "\n",
    "# features to drop is category 1 & category 2\n",
    "features_to_drop = features_not_predictive + features_to_remove\n",
    "# and the outcome:\n",
    "label = \"lead_to_readmission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = df.gender.astype('category')\n",
    "df['age_group']= df.age_group.astype('category')\n",
    "df['mainDiagBlock'] = df.mainDiagBlock.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and the target variable\n",
    "\n",
    "X = df.drop(columns=features_to_drop+[label])\n",
    "y = df[[label]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the patients (20% in test set, 60% in train and 10% in validation set)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "patient_list = pd.unique(df['patientID'])\n",
    "\n",
    "patient_train, patient_test = train_test_split(patient_list, test_size=0.3, random_state=42) #70 percent test data\n",
    "patient_val, patient_test = train_test_split(patient_test, test_size=0.6, random_state=42) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_to_readmission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207253</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207254</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207255</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207256</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207258</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144651 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lead_to_readmission\n",
       "1                       0.0\n",
       "2                       0.0\n",
       "4                       0.0\n",
       "5                       0.0\n",
       "6                       1.0\n",
       "...                     ...\n",
       "207253                  0.0\n",
       "207254                  0.0\n",
       "207255                  0.0\n",
       "207256                  0.0\n",
       "207258                  0.0\n",
       "\n",
       "[144651 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the dataset\n",
    "\n",
    "train_data = X[X['patientID'].isin(patient_train)]\n",
    "val_data = X[X['patientID'].isin(patient_val)]\n",
    "test_data = X[X['patientID'].isin(patient_test)]\n",
    "\n",
    "y = y.join(X['patientID'])\n",
    "\n",
    "train_y = y[y['patientID'].isin(patient_train)]\n",
    "val_y = y[y['patientID'].isin(patient_val)]\n",
    "test_y = y[y['patientID'].isin(patient_test)]\n",
    "\n",
    "train_y = train_y.drop('patientID', axis=1)\n",
    "val_y = val_y.drop('patientID', axis=1)\n",
    "test_y = test_y.drop('patientID', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode categorical variables for train and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot_age_group =  pd.get_dummies(train_data['age_group'])\n",
    "\n",
    "train_data = train_data.join(oneHot_age_group)\n",
    "train_data  = train_data.drop('age_group', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot_main_diag =  pd.get_dummies(train_data['mainDiagBlock'])\n",
    "\n",
    "train_data = train_data.join(oneHot_main_diag)\n",
    "train_data  = train_data.drop('mainDiagBlock', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot_gender =  pd.get_dummies(train_data['gender'])\n",
    "\n",
    "train_data = train_data.join(oneHot_gender)\n",
    "train_data  = train_data.drop('gender', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot_urgency =  pd.get_dummies(train_data['degree_of_urgency'],prefix=['planned','emergency'])\n",
    "\n",
    "train_data = train_data.join(oneHot_urgency)\n",
    "train_data  = train_data.drop('degree_of_urgency', axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot_age_group =  pd.get_dummies(test_data['age_group'])\n",
    "\n",
    "test_data = test_data.join(oneHot_age_group)\n",
    "test_data  = test_data.drop('age_group', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot_main_diag =  pd.get_dummies(test_data['mainDiagBlock'])\n",
    "\n",
    "test_data = test_data.join(oneHot_main_diag)\n",
    "test_data  = test_data.drop('mainDiagBlock', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot_gender =  pd.get_dummies(test_data['gender'])\n",
    "\n",
    "test_data = test_data.join(oneHot_gender)\n",
    "test_data  = test_data.drop('gender', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHot_urgency =  pd.get_dummies(test_data['degree_of_urgency'],prefix=['planned','emergency'])\n",
    "\n",
    "\n",
    "test_data = test_data.join(oneHot_urgency)\n",
    "test_data  = test_data.drop('degree_of_urgency', axis = 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(c) Train a logistic regression model to predict mortality based on the standardized features.\n",
    "\n",
    "Compute and report the following performance metrics in a table on each of the train and\n",
    "test sets:\n",
    "\n",
    "- Accuracy\n",
    "\n",
    "- Precision\n",
    "\n",
    "- Recall\n",
    "\n",
    "- F1-score\n",
    "\n",
    "- AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Miniconda\\lib\\site-packages\\sklearn\\utils\\validation.py:1675: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['float', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Miniconda\\lib\\site-packages\\sklearn\\utils\\validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Miniconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(train_data.drop(['patientID',train_data.keys()[-1], train_data.keys()[-2]],axis = 1), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a model on train and test set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "metrics_dic = {\"Accuracy\": accuracy_score,\n",
    "               \"Precision\": precision_score,\n",
    "               \"Recall\": recall_score,\n",
    "               \"F1-score\": f1_score,\n",
    "               \"AUC Score\": roc_auc_score}\n",
    "\n",
    "\n",
    "def evaluate_model(model, metrics_dic, X_train, y_train, X_test, y_test, sample_weight=None):\n",
    "    metrics = defaultdict(list)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    for metric_name, metric_fun in metrics_dic.items():\n",
    "        metrics[metric_name].append(metric_fun(y_train, pred_train, sample_weight=sample_weight))\n",
    "        metrics[metric_name].append(metric_fun(y_test, pred_test))\n",
    "\n",
    "    metrics[\"Dataset\"] = [\"Train\", \"Test\"]\n",
    "    # Result as a dataframe\n",
    "    res = pd.DataFrame()\n",
    "    for key in list(metrics.keys()):\n",
    "        res[key] = metrics[key]\n",
    "    res.set_index(\"Dataset\", inplace=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Miniconda\\lib\\site-packages\\sklearn\\utils\\validation.py:1675: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['float', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Miniconda\\lib\\site-packages\\sklearn\\utils\\validation.py:1675: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['float', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr_metrics = evaluate_model(lr, metrics_dic, train_data.drop(['patientID',train_data.keys()[-1], train_data.keys()[-2]],axis=1), train_y, test_data.drop(['patientID',train_data.keys()[-1], train_data.keys()[-2]],axis=1), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.634859</td>\n",
       "      <td>0.132601</td>\n",
       "      <td>0.645494</td>\n",
       "      <td>0.220007</td>\n",
       "      <td>0.639716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.629394</td>\n",
       "      <td>0.129268</td>\n",
       "      <td>0.620859</td>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.625504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Accuracy  Precision    Recall  F1-score  AUC Score\n",
       "Dataset                                                    \n",
       "Train    0.634859   0.132601  0.645494  0.220007   0.639716\n",
       "Test     0.629394   0.129268  0.620859  0.213983   0.625504"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_metrics.to_csv(\"lr_metrics_no_urgency.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity :  0.6301485091077851\n",
      "Specificity :  0.6208592981305346\n",
      "[[21725 12751]\n",
      " [ 1156  1893]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Miniconda\\lib\\site-packages\\sklearn\\utils\\validation.py:1675: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['float', 'str']. An error will be raised in 1.2.\n",
      "  Note that passing sample_weight=None will output an array of ones.\n"
     ]
    }
   ],
   "source": [
    "#Produce confusion matrix and sensitivity/specificity\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "pred = lr.predict(drop_gender)\n",
    "\n",
    "#print(classification_report(test_y,pred))\n",
    "\n",
    "cm1 = confusion_matrix(test_y,pred)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1]) \n",
    "print('Sensitivity : ', sensitivity1 )  \n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1]) \n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "print(cm1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a8029a51b57fe2d11fbd3b82c1cfa41a63a260330aa68039b758425bc8dfc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
